Traceback (most recent call last):
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/.venv/bin/lm_eval", line 5, in <module>
    from lm_eval.__main__ import cli_evaluate
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/lm_eval/api/registry.py", line 6, in <module>
    from lm_eval.api.model import LM
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/lm_eval/api/model.py", line 8, in <module>
    import transformers
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/.venv/lib/python3.10/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/.venv/lib/python3.10/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/.venv/lib/python3.10/site-packages/transformers/utils/__init__.py", line 27, in <module>
    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema
  File "/home/ubuntu/us-west-3/lm-evaluation-harness/.venv/lib/python3.10/site-packages/transformers/utils/chat_template_utils.py", line 39, in <module>
    from torch import Tensor
ImportError: cannot import name 'Tensor' from 'torch' (unknown location)
Start time: Mon Nov 25 23:02:09 UTC 2024
Running command: echo test
test

real	0m0.000s
user	0m0.000s
sys	0m0.000s
End time: Mon Nov 25 23:02:09 UTC 2024
Start time: Mon Nov 25 23:06:35 UTC 2024
Running command: lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B-Instruct,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.8,data_parallel_size=1,max_model_len=8192,add_bos_token=True,seed=42 --tasks meta_math --batch_size auto --output_path eval_results --seed 42 --log_samples
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
2024-11-25:23:08:13,495 INFO     [__main__.py:279] Verbosity set to INFO
